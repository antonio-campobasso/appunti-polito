# Data Mining
Estrazione automatica (tramite appositi algoritmi) di informazioni, rappresentate come modelli astratti (**pattern**).

*es.* profilazione(amazon, facebook, maps).

## Knowledge discovery process
![[schema ma 28_09 - 10_18.excalidraw]]

- **Preprocessing**: sempre necessario, dato che tutti i dati sono "dirty".
- **Regole di associazione**: necessarie ad estrarre correlazioni o pattern da *database transazionali*.
- **Classificazione**: definizione di un modello dato un fenomeno e utilizzo del modello per classificare dati futuri.
- **Clustering**: individuazione di gruppi di dati simili e allo stesso tempo di eccezioni (*outliners*).

### Preprocessing
Tipi di attributo:
- **Nominali** (codice fiscale, colore degli occhi, codice postale)
	- Posso dire solo se sono uguali o diversi.
- **Ordinale** ( voti, ranking, taglia di un capo)
	- Posso anche definire un ordine (precede o segue).
- **Intervallo** (data)
	- Posso anche definire una differenza/somma.
- **Ratio** (temperature in kelvin, lunghezza, tempo, conto)
	- Posso fare anche moltiplicazioni e rapporti.

- **Discreto** (insieme definito di valori)
	- spesso rappresentati con interi.
- **Continuo** (definito con valori reali)
	- sono di tipo ratio.

#### Data set
**Tabelle** (collezioni di record)
	- Ogni record è caratterizzato da un insieme fisso di attributi.
	
**Dati documentali** (dati testuali semi-strutturati o non strutturati)
	- Ogni documento diventa un vettore di termini (tabella di conto di parole, con attributi corrispondenti alle parole e i documenti come record).
**Dati transazionali**
	- Tipo speciale di record data, ogni record include un insieme di oggetti(es. scontrini).
**Dati grafo**
**Dati ordinati(sequenze)**

### Data preparation
**Aggregazione** = combinare 2 o più attributi(oppure oggetti) in un singolo attributo (oppure oggetto).

**Data reduction** = genera una rappresentazione ridotta del dataset, più piccola di volume ma può fornire gli stessi risultati analitici.

**Sampling** = tecnica principale per selezione dati:
- Simple Random sampling = stessa probabilità di selezionare un certo oggetto.
	- con rimpiazzamento = il campione estratto viene copiato e reinserito nel set.
- Stratificato = i dati vengono divisi in diverse partizioni e si sceglie un valore randomico da ogni partizione.

**Dimensionality reduction**
- PCA = cercare una proiezione che cattura la più grande variazione tra i dati.
- Feature subset selection = si eliminano attributi ridondanti e informazioni irrilevanti.
	- **Brute force** = prova tutti i possibili sottoinsiemi come input all'algoritmo di data mining.
	- **Embedded** = la feature selection avviene come parte dell'algoritmo di data mining.
	- **Filtro** = le feature sono selezioante prima dell'applicazione dell'algoritmo.
	- **Wrapper** = uso l'algoritmo come black box per trovare il miglior sottoinsieme.

**Feature creation** = crea nuovi attributi che possono catturare le informazioni più importanti di un data set in maniera più efficente rispetto agli attributi originali.
	- **Feature extraction** = specifici per il dominio.
	- **Mapping** data to new space.
	- **Feature construction** = combinare gli attributi.

**Discretizzazione** = trasformare valori continui in valori ordinabile.
- N intervalli della stessa lunghezza
	- *facile* da implementare.
	- può essere *distorto* da outliers e dati sparsi.
	- approccio *incrementale*.
- N intervalli con la stessa cardinalità (approsimativamente)
	- risolve il problema degli outliner.
	- approccio *non incrementale*.
- Clustering
	- gestisce bene dati sparsi e outliers.
- Analisi della distribuzione dei dati

**Binarizzazione** = mappa un attributo categorico in uno o più valori.

**Normalizzazione** = il valore di un attributo è scalato per appartenere ad un intervallo più piccolo.

#### Somiglianza e distanza
la *somiglianza* è una misura numerica che indica quanto due oggetti sono simili, spesso compresa tra 0 e 1.

La *distanza* (o dissomiglianza) indica quanto 2 oggetti sono diversi, ha valore minimo 0 (nel caso di oggetti uguali) e valore massimo variabile.

Per attributi nominali ho solo 2 valori possibili di somiglianza e distanza (0 e 1).

per i valore ordinali
- somiglianza = 1 - d
- distanza = |x-y|/(n-1)

Per calcolare la distanza euclidea bisogna normalizzare gli attributi.

Posso generare la distanza euclidea con la distanza di **Minkowski**, invece di elevare al quadrato i termini ed estrarre la radice quadrata, si considera un parametro r da sostituire:
- r = 1 --> city block (manhattan, taxi cab, L1, norm).
- r = 2 --> distanza euclidea.
- r -> $\infty$ --> supremum (Lmax nrom, Linf norm)
	- considera solo la componente a distanza massima.

Una distanza è **metrica** se
- è sempre maggiore o uguale a 0 e vale 0 solo se x e y sono uguali (definita positivamente).
- è simmetrica.
- vale la disuguaglianza trangolare.

distanza di *mahalanobis* = (x-y)Trasposto per la matrice di covarianza per (x-y).
	- legata alla densità, 2 punti sono più vicini se ci sono più punti tra di loro.

### Associazione
#### Correlazione dati


- Misura la relazione lineare tra 2 dati.
- Utile durante la fase di *data exploration*.
- Analisi degli attributi di correlazione.

#### Definizioni
- **Support count (#)** = frequenza di occorrenza di un itemset.
- **Support (sup)** = frazione di transazioni che contengono l'itemset.
- **Frequent Itemset** = se il suo supporto è maggiore o uguale ad una certa soglia (minsup).
- **Confidenza** = data l'associazione A=>B allora la frequenza di B nelle transazioni contenenti A vale $\frac{sup(A,B)}{sup(A)}$
	- probabilità condizionale di trovare B avendo trovato A.
	- "forza" della "=>" .

Dato un set di transazioni T, il mining di regole di associazione è l'estrazione delle regole che soddisfano i seguenti requisiti:
- Supporto >= minsup.
- Confidenza >= minconf.
Il risultato è:
- completo (tutte le regole soddisfano tutti i requisiti).
- corretto (solo le regole che soddisfano tutti i requisiti).

#### regole di associazione
1. Estrazione di itemset frequenti
	1. Diverse tecniche (vedi sotto).
	2. Molto dispendioso in termini computazionali (l'estrazione viene limitata con support treshold).
2. Estrazione delle regole di associazione
	1. Si generano tutte le possibili partizioni degli itemset frequenti.

##### Principio a priori
> Se un itemset è frequente, allora anche i suoi subset devono essere frequenti.

Se il supporto di un itemset non rispetta la soglia imposta, non cerco gli altri sottoinsiemi inclusi in esso. 2 step principali:
1. Generazione candidati
	1. Si generano i candidati di lunghezza k+1 attraverso il koin di itemset frequenti di lunghezza k.
	2. Si applica il principio a priori e scarto gli itemset di lunghezza k+1 che contengono almeno un k-itemset non frequente.
2. Generazione degli itemset frequenti.
	1. Scan del db per ottenere il supporto dei candidati k+1.
	2. Elimina i candidati sotto minsup.

Le scansioni del database sono tante (n+1 con n = longest frequent pattern lenght) e ogni transazione può contenere diversi candidati
è possibile aumentare l'efficenza tramite l'uso degli *hash tree*.

##### Frequent pattern growth
Sfrutta una rappresentazione compressa del database, il frequent pattern tree:
	- Alta compressione per dati densi.
	- Rappresentazione completa per frequent pattern mining.
	- Visita ricorsiva del FP-tree.
	- Metodo divide and conquer.
	- Solo 2 database scan(conta il supporto, costruisci l'albero)
		- Si costruisce una header table, ordinando i singoli elementi per il loro supporto.
		- Si esegue lo scan del database e i subset vengono ordinati secondo la header table es. {a,b} -> {b,a}.
		- Inserisco i singoli elementi ordinati.
		- Se inserisco un elemento già presente aumento il suo valore di 1.

Per il processo di estrazione si usa la conditional pattern base (CPB), spesso indicata come X-CPB con X corrispondente al valore con supporto più basso nella header table
	- Una volta scelto l'elemento da estrarre ricerco nell'albero tutti i nodi corrispondenti, collegati tramite un item pointer.
	- risalgo l'albero ed estraggo l'itemset senza X, con supporto pari al valore di X presente nell'albero.

Da questa tabella posso riformare un fp-tree e ricominciare ricorsivamente (X-CPB diventa XY-CPB e così via).

Ogni volta che costruiamo un CPB estraiamo il valore da ricercare e lo indichiamo come frequent itemset.

#### Altre definizioni
Itemset frequente massimale = se nessuno dei suoi superset immediati è frequente (itemset più lunghi che sono frequenti).

Itemset chiuso = se nessuno dei suoi superset ha lo stesso supporto dell'itemset.

Tutti i massimali sono anche closed.

considerando A => B:
	**correlazione** (o *lift*) = rapporto tra confidenza di una relazione e supporto di B.
		- **Indipendenza statistica** se valore = 1.
		- **Correlazione positiva** se valore > 1.
		- **Correlazione negativa** se valore < 1.
		
##### Peso
Si considera un peso associato all'item/transazione.

Posso introdurre gerarchie (**tassonomia**).
Il *data item* è l'istanza nel dataset e rappresenta concetti dettagliati.
*Oggetto generalizzato* = Aggregazione in concetto di alto livello, rappresenta astrazione delle istanze.
