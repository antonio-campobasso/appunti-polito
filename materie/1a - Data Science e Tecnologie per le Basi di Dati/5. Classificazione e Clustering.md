### Classificazione
#### Alberi di decisione
Diversi algoritmi per costruirne uno, analizziamo algoritmo di *Hunt*
	- Se il dataset contiene record che appartengono a più classi:
		- Seleziona il miglior attributo A sul quale splittare il dataset e crea il nodo A.
		- Dividi il dataset in sottoinsieme applica ricorsivamente la procedura ad ogni sottoinsieme.
	- Se il dataset contiene record che appartengono alla stessa classe yt:
		- Allora il nodo è una foglia, nominata yt.
	- Se il dataset è vuoto:
		- allora il nodo è una foglia, chiamato default (yd).

Adotta strategia *greedy*:
	- Si sceglie ad ogni step un ottimo locale.

Gli attributi vengono scelti in modo da generare partizioni più omogenee possibili:
	- Stessi valori di classi.
	- Viene misurato tramite l'*impurità*.

#### GINI
per un nodo t:
$$GINI(t) = 1 - \sum{[p(j|t)]^2}$$

Con $p(j|t)$ uguale alla frequenza della classe j nel nodo t:
	- **Purezza totale** -> gini = 0.
	- **Impurità massima** -> gini = $1 - 1/nc$ con nc = numero classi.
		- I record sono egualmente distribuiti tra tutte le classi.

Per splittare in base al gini index:
$$GINIsplit = \sum{\frac{ni}{n} GINI(i)}$$

Con i che va da 1 a k = numero partizioni
	- ni = numero di record al figlio i.
	- n = numero di record al nodo p.

##### GINI con attributi continui
Per ogni attributo
- Sort dei valori dell'attributo.
- Scan lineare dei valori, ogni volta aggiorna la matrice di count e calcola il gini index.
- Scegli la split position che ha il gini index più basso.

#### Random forest
Insieme di alberi di decisione. Dal dataset si generano sottoinsiemi random, ad ogni decisione scelgo un random subset di attributi, gli alberi sono scorrelati.

#### Rule based
Predico la classe di un record attraverso una serie di regole.

#### k-nearest neighbor
A partire da un insieme di record classificati, predico la classe di un nuovo caso in base alla distanza dagli altri record.

Necessita:
- Un insieme di record.
- Distanza metrica per calcolare la distanza tra i record.
- Il valore di k, il numero di vicini più prossimi da recuperare.

Per classificare un record sconosciuto:
- Computa la distanza dagli altri training records.
- Identifica i k vicini più prossimi.
- Usa etichette di classe dei vicini prossimi per determinare la classe del record sconosciuto.

I voti per determinare la classe vengono attenuati in base all'inverso della distanza al quadrato. Se mi trovo in un area con k vicini allora conto il voto (espresso come classe di appartenenza), normalizzato.

#### Classificazione bayesiana
- C = qualsiasi label di classe.
- X = < x1,...,xk > record da classificare.

1. Computa P(C|X) per tutte le classi (Probabilità che il record X appartenga a C).
2. Assegna X alla classe con il P(C|X) massimale.

Applica il teorema di Bayes:
$$ P(C|X) = P(X|C) * P(C) / P(X)$$

- P(X) costante per tutti C
- P(C) probabilità a priori di C = Nc/N

Ipotesi naive di indipendenza statistica degli attributi.
Per attributi discreti P(xk|C) = |xkc|/Nc

#### Support Vector Machines
Trovare l'iperpiano che massimizza il margine, ovvero la distanza minima tra gli elementi separati. È possibile adottare un approccio non lineare.

#### Reti neurali
Ispirato dalla struttura del cervello umano:
	- Neuroni = centri di elaborazione.
	- Sinapsi = collegamenti tra neuroni.

#### Valutazione di un modello
- Stima affidabile.
- Performance:
	- Distribuzione classi.
	- Costo di assegnazione errata di classe.
	- Dimensione di training set e test set.

#TODO vedi slides

### Clustering
Ricerca di gruppi di oggetti simili tra di loro:
	- Minimizzare distanze tra elementi dello stesso cluster.
	- Massimizzare distanze tra cluster diversi.

*Clustering = insieme di cluster.*

**Clustering partizionale**:
	- Gli oggetti sono suddivisi in sottoinsiemi non sovrapposti tra loro.

**Clustering gerarchico**:
	- I cluster sono nidificati secondo l'ordine di aggregazione.
	- Per rappresentare l'ordine si utilizza anche un dendrogramma.

Altre distinzioni:

- **Esclusivo** vs non esclusivo
	- Nel non esclusivo i punti possono appartenere a diversi cluster.

- **Fuzzy** vs non fuzzy
	- Nel fuzzy tutti i punti appartengono ad ogni cluster con un peso compreso tra 0 e 1.
	- La somma ti tutti i pesidi un punto deve essere 1.
	- Simile a clustering probabilistico.

- **Parziale** vs **Completo**:
	- in alcuni casi vogliamo i cluster di una parte dei dati.

- **Eterogeneo** vs **Omogeneo**
	- I cluster sono di diverse dimensioni, forme e densità.

#### Tipi di cluster
**Ben separati**:
	- Ogni punto in un cluster è più vicino ad ogni altro punto del cluster rispetto ad ogni altro punto al di fuori del cluster.

**Center-based**:
	- Ogni punto in un cluster è più vicino al "centro" del cluster a cui appartiene rispetto al centro di ogni altro cluster.
	- Il centro del cluster può essere un *centroide* (dato dalla media di tutti i punti del cluster) o un *medioide* (il più "rappresentativo").

**Contiguity-based**:
	- Contagious cluster (nearest neighbor/transitive) = ogni punto è più vicino ad uno o più punti nel cluster rispetto ad ogni altro punto fuori dal cluster.

**Density-based**:
	- Un cluster è una regione densa di punti, separata da regioni a bassa densità da altre regioni ad alta densità.
	- Uitle quando i cluster sono irregolari o intersecati e quando sono presenti rumore e outliers.

#### Algoritmi di clustering
##### k-means e varianti
- Approccio clustering partizionale.
- Ad ogni cluster è associato un centroide.
- Ogni punto è assegnato al cluster con il centroide più vicino.
- K da specificare = nuimero di cluster.
- Algoritmo di base molto semplice.
- Ha problemi con cluster non omogenei e con cluster a densità differenti.

1. Seleziona k punti come centroidi di partenza.
2. **do**
3. Forma K cluster assegnando tutti i punti ai centroidi più vicini.
4. Ricalcola il centroide all'interno di ogni cluster.
5. **while** i centroidi non cambiano più.

SSE = Sum of Squared Error
$$SSE = \sum_{i=1}^K{\sum_{x\in C_i}dist^2*(m_i, x)}$$

**Pre-processing**:
	- Normalizzare i dati.
	- Eliminare gli outliers.

**Post-processing**:
	- Eliminare piccoli cluster che potrebbero rappresentare gli outliers.
	- Separare i cluster meno densi e unire quelli più densi.
	- Questi step possono essere usati anche durante la fase di clustering.

**Bisecting k-means**
	1. Inizializza lista di cluster con un cluster che contiene tutti i punti.
	2. **do**
	3. Seleziona un cluster dalla lista.
	4. **for** i = 1 **to** numero_iterazioni.
	5. Dividi il cluster selezionato in 2 utilizzando k-means.
	6. **end for**
	7. Aggiungi i 2 cluster con il SSE più basso alla lista dei cluster.
	8. **while** la lista dei cluster contiene k cluster.
	
##### Clustering gerarchico
2 tipi principali:
- **Agglomerativo**
	- Inizia con i punti come cluster individuali.
	- Ad ogni step merge della coppia di cluster più vicini finchè non rimangono k cluster.
- **Divisivo**
	- Inizia con un cluster contenente tutti i punti
	- Ad ogni step dividi il cluster finchè non non rimangono k cluster.

Gli algoritmi gerarchici usano matrice di distanza.

|     | p1  | p2  | p3  | p4  | p5  |
| --- | --- | --- | --- | --- | --- |
| p1  |     |     |     |     |     |
| p2  |     |     |     |     |     |
| p3  |     |     |     |     |     |
| p4  |     |     |     |     |     |
| p5  |     |     |     |     |     |

Quando si effetua l'unione tra due cluster si sostituiscono le righe e le colonne dei cluster coinvolti.

Distanza tra cluster:
- **Min** (single link)
	- riesce a gestire cluster con cardinalità diversa.
	- molto sensibile al rumore.
- **Max**
	- Gestisce meglio con il rumore.
	- Non riesce a gestire cluster con cardinalità diversa.
- **Media di gruppo**
	- compromesso tra *min* e *max*.
- **Distanza tra centroidi**
- **Metodo di ward** (SSE)
	- Misura la somiglianza tra 2 cluster in base al valore dell'errore quadratico medio (ricerca del minimo).
	- Può essere usato per inizializzare il k-means.

Spazio necessario $O(N^2)$, tempo $O(N^3)$

#### Clustering density-based
DBSCAN = algoritmo basato sulla densità (numero di punti in un raggio specificato Eps)
	- Un punto è un punto core se ha più del numero di punti specificato MinPts entro un Eps.
	- Un punto di frontiera ha meno MinPts entro un Eps, ma è vicino ad un punto core.
	- Un punto di rumore è un punto che non è nè core, nè di frontiera.

##### Misure interne
- Coesione cluster (basata su SSE, interno ad un cluster):

$$ WSS = \sum_{i}{\sum_{x\in{C_i}}{(x-m_i)^2}}$$


- Separazione cluster (tra cluster diversi):

$$ BSS = \sum_i{|C_i|(m-m_i)^2}$$


- Silhouette (valuta quanto bene ogni oggetto appartiene ad un certo cluster, considera coesione e separazione e può essere computato per punti individuali, cluster individuali, risultati di clustering):

$$ s(i) = \frac{b(i)-a(i)}{max\{a(i),b(i)\}}$$ 

- Range tra -1 e 1 (tipicamente tra 0 e 1, valori ottimi vicini ad 1).
- a(i) = media di distanze di i con tutti gli altri oggetti dello stesso cluster.
- b(i) = minimo della media di distanze tra i e ogni altro cluster a cui i non appartiene.
- la media di s(i) su tutti i dati di un cluster misura quanto sono raggruppati i dati nel cluster.
- la media di s(i) sul tutti i dati del dataset misura quanto sono appropriati i dati di cui si è effettuato il clustering.


- Indice di Rand:

$$ Rand Index = \frac{f_{00} + f_{11}}{f_{00} + f_{01} + f_{10} + f_{11}}$$

- Idea: gli oggetti che sono nello stesso cluster dovrebbero essere nella stessa classe e viceversa.
- f00 = numero di coppie di oggetti che hanno diferse classi e cluster diversi.
- f01 = numero di coppie di oggetti che hanno diverse classi e lo stesso cluster.
- f10 = numero di coppie di oggeti che hanno la stessa classe e cluster diversi.
- f11  = numero di coppie di oggetti cge hanno la stessa classe e lo stesso cluster.